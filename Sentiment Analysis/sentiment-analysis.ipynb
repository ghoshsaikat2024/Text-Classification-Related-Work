{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T15:56:37.328448Z","iopub.status.busy":"2024-07-06T15:56:37.328049Z","iopub.status.idle":"2024-07-06T15:56:40.507796Z","shell.execute_reply":"2024-07-06T15:56:40.506590Z","shell.execute_reply.started":"2024-07-06T15:56:37.328415Z"},"trusted":true},"outputs":[],"source":["import nltk\n","import pandas as pd\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk import WordNetLemmatizer\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics import accuracy_score, classification_report\n","nltk.download('wordnet') #For using WordNetLemmatizer\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T15:56:44.560145Z","iopub.status.busy":"2024-07-06T15:56:44.559566Z","iopub.status.idle":"2024-07-06T15:56:44.565645Z","shell.execute_reply":"2024-07-06T15:56:44.564319Z","shell.execute_reply.started":"2024-07-06T15:56:44.560108Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T15:56:58.936750Z","iopub.status.busy":"2024-07-06T15:56:58.936343Z","iopub.status.idle":"2024-07-06T15:56:59.146080Z","shell.execute_reply":"2024-07-06T15:56:59.145021Z","shell.execute_reply.started":"2024-07-06T15:56:58.936718Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset shape is: (40000, 4)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>sentiment</th>\n","      <th>author</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1956967341</td>\n","      <td>empty</td>\n","      <td>xoshayzers</td>\n","      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1956967666</td>\n","      <td>sadness</td>\n","      <td>wannamama</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1956967696</td>\n","      <td>sadness</td>\n","      <td>coolfunky</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1956967789</td>\n","      <td>enthusiasm</td>\n","      <td>czareaquino</td>\n","      <td>wants to hang out with friends SOON!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1956968416</td>\n","      <td>neutral</td>\n","      <td>xkilljoyx</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     tweet_id   sentiment       author  \\\n","0  1956967341       empty   xoshayzers   \n","1  1956967666     sadness    wannamama   \n","2  1956967696     sadness    coolfunky   \n","3  1956967789  enthusiasm  czareaquino   \n","4  1956968416     neutral    xkilljoyx   \n","\n","                                             content  \n","0  @tiffanylue i know  i was listenin to bad habi...  \n","1  Layin n bed with a headache  ughhhh...waitin o...  \n","2                Funeral ceremony...gloomy friday...  \n","3               wants to hang out with friends SOON!  \n","4  @dannycastillo We want to trade with someone w...  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data=pd.read_csv(\"/kaggle/input/sentiment-dataset/text_emotion.csv\")\n","print(\"Dataset shape is:\",data.shape)\n","data.head()"]},{"cell_type":"markdown","metadata":{},"source":["***Here we can see that data imbalancing exist.So to get good results we need to use handle the imbalance.***"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T15:57:03.137753Z","iopub.status.busy":"2024-07-06T15:57:03.137344Z","iopub.status.idle":"2024-07-06T15:57:03.158214Z","shell.execute_reply":"2024-07-06T15:57:03.156764Z","shell.execute_reply.started":"2024-07-06T15:57:03.137720Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Types of sentiments are:\n"," sentiment\n","neutral       8638\n","worry         8459\n","happiness     5209\n","sadness       5165\n","love          3842\n","surprise      2187\n","fun           1776\n","relief        1526\n","hate          1323\n","empty          827\n","enthusiasm     759\n","boredom        179\n","anger          110\n","Name: count, dtype: int64\n"]}],"source":["sentiment_types=data[\"sentiment\"].value_counts()\n","print(\"Types of sentiments are:\\n\",sentiment_types)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T15:57:08.453885Z","iopub.status.busy":"2024-07-06T15:57:08.453502Z","iopub.status.idle":"2024-07-06T15:57:08.475681Z","shell.execute_reply":"2024-07-06T15:57:08.474220Z","shell.execute_reply.started":"2024-07-06T15:57:08.453856Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Types of sentiments are:\n"," sentiment\n","0     8638\n","1     8459\n","2     5209\n","3     5165\n","4     3842\n","5     2187\n","6     1776\n","7     1526\n","8     1323\n","9      827\n","10     759\n","11     179\n","12     110\n","Name: count, dtype: int64\n"]}],"source":["#Changing the categorical variables into numeric form.\n","mapping={\"neutral\":0,\"worry\":1,\"happiness\":2,\"sadness\":3,\"love\":4,\"surprise\":5,\"fun\":6,\"relief\":7,\"hate\":8,\"empty\":9,\"enthusiasm\":10,\"boredom\":11,\"anger\":12}\n","data[\"sentiment\"]=data[\"sentiment\"].map(mapping)\n","\n","#Verifying.\n","sentiment_types=data[\"sentiment\"].value_counts()\n","print(\"Types of sentiments are:\\n\",sentiment_types)"]},{"cell_type":"markdown","metadata":{},"source":["***Now we create a function to tokenize the tweets,remove stopwords(i.e frequently used insignificant words).***"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T15:57:29.496249Z","iopub.status.busy":"2024-07-06T15:57:29.495203Z","iopub.status.idle":"2024-07-06T15:57:29.522864Z","shell.execute_reply":"2024-07-06T15:57:29.521738Z","shell.execute_reply.started":"2024-07-06T15:57:29.496211Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'hello not so good morning'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["def process_text(text:str):\n","    '''\n","    This function tokenizes the tweets, removes numeric char based words ,converts all the chars into lowercase and then lemmatizes the words.\n","    The reason we use lemmatization is ,it uses wordnet library to look up origin of a word.Like better/best will be converted to good.\n","    Here we should not use stop words as it will remove words like not/a/very ,since these kind of words which are important for sentiment classification.\n","    Note:Read about Lemmatization and Stemming and their difference .(Stemming just chops last section of the word)\n","    '''\n","    # stop_words=set(stopwords.words('english'))\n","    word_lemmatizer=WordNetLemmatizer()  #It's a class\n","    tokens=word_tokenize(text)\n","    tokens=[word for word in tokens if word.isalpha() ]  #and len(word)>=2\n","    tokens=[word.lower() for word in tokens ]\n","    # tokens=[word for word in tokens if word not in stop_words]\n","    tokens=[word_lemmatizer.lemmatize(word) for word in tokens ]\n","\n","    return ' '.join(tokens)\n","\n","#Veryfying the function\n","text=\"Hello! not  so Good Morning.\"\n","process_text(text)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T15:57:32.415358Z","iopub.status.busy":"2024-07-06T15:57:32.414920Z","iopub.status.idle":"2024-07-06T15:57:44.907069Z","shell.execute_reply":"2024-07-06T15:57:44.905833Z","shell.execute_reply.started":"2024-07-06T15:57:32.415324Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>sentiment</th>\n","      <th>author</th>\n","      <th>content</th>\n","      <th>processesed tweets</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1956967341</td>\n","      <td>9</td>\n","      <td>xoshayzers</td>\n","      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n","      <td>tiffanylue i know i was listenin to bad habit ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1956967666</td>\n","      <td>3</td>\n","      <td>wannamama</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","      <td>layin n bed with a headache ughhhh waitin on y...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1956967696</td>\n","      <td>3</td>\n","      <td>coolfunky</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","      <td>funeral ceremony gloomy friday</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1956967789</td>\n","      <td>10</td>\n","      <td>czareaquino</td>\n","      <td>wants to hang out with friends SOON!</td>\n","      <td>wants to hang out with friends soon</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1956968416</td>\n","      <td>0</td>\n","      <td>xkilljoyx</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","      <td>dannycastillo we want to trade with someone wh...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     tweet_id  sentiment       author  \\\n","0  1956967341          9   xoshayzers   \n","1  1956967666          3    wannamama   \n","2  1956967696          3    coolfunky   \n","3  1956967789         10  czareaquino   \n","4  1956968416          0    xkilljoyx   \n","\n","                                             content  \\\n","0  @tiffanylue i know  i was listenin to bad habi...   \n","1  Layin n bed with a headache  ughhhh...waitin o...   \n","2                Funeral ceremony...gloomy friday...   \n","3               wants to hang out with friends SOON!   \n","4  @dannycastillo We want to trade with someone w...   \n","\n","                                  processesed tweets  \n","0  tiffanylue i know i was listenin to bad habit ...  \n","1  layin n bed with a headache ughhhh waitin on y...  \n","2                     funeral ceremony gloomy friday  \n","3                wants to hang out with friends soon  \n","4  dannycastillo we want to trade with someone wh...  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["#Applying the process_text function on the tweet column\n","data['processesed tweets']=data['content'].apply(process_text)\n","data.head()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Bag of Words based feature vector shape:(40000, 243282)\n","\n"," TF-IDF based feature vector shape:(40000, 243282)\n"]}],"source":["#Creating feature vector from text using Bag of Words with bigram model .\n","'''In unigram we just take the feature as occurance of a particular word .In ngram we take feature as the occurence of a sequence of n words ,which help us in getting some context.\n","For example I'm not feeling good sentence is taken as a bigram or trigram model manner then we get the  context of negation otherwise individual word occurence like in unigram does not \n","mean anything.\n","Note:To know more read about N-gram model.\n","'''\n","bg_vectorizor=CountVectorizer(ngram_range=(1,2))\n","X_bow=bg_vectorizor.fit_transform(data['processesed tweets'])\n","\n","#Label.\n","Y=data['sentiment']\n","\n","#Creating feature vector from text using Tf-Idf with bigram model.\n","tf_vectorizor=TfidfVectorizer(ngram_range=(1,2))\n","X_tf=tf_vectorizor.fit_transform(data['processesed tweets'])\n","\n","print(f'Bag of Words based feature vector shape:{X_bow.shape}\\n TF-IDF based feature vector shape:{X_tf.shape}')"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["#Train test split for vectors created using bag of words\n","X_bow_train,X_bow_test,Y_train,Y_test=train_test_split(X_bow,Y,test_size=0.25,random_state=42)\n","#Train test split for vector/features created using TF_IDF\n","X_tf_train,X_tf_test,Y_train,Y_test=train_test_split(X_tf,Y,test_size=0.25,random_state=42)"]},{"cell_type":"markdown","metadata":{},"source":["***Classifying using Logistic Regression***"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/saikat/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","\n","  warnings.warn(\n","\n","/home/saikat/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Bag of Words Accuracy: 0.3305\n","\n","              precision    recall  f1-score   support\n","\n","\n","\n","           0       0.39      0.50      0.44      2183\n","\n","           1       0.35      0.33      0.34      2093\n","\n","           2       0.34      0.34      0.34      1288\n","\n","           3       0.32      0.32      0.32      1314\n","\n","           4       0.43      0.43      0.43       941\n","\n","           5       0.15      0.13      0.14       516\n","\n","           6       0.13      0.11      0.12       421\n","\n","           7       0.20      0.14      0.16       435\n","\n","           8       0.29      0.28      0.29       332\n","\n","           9       0.04      0.03      0.03       203\n","\n","          10       0.01      0.00      0.01       204\n","\n","          11       0.04      0.02      0.03        43\n","\n","          12       0.00      0.00      0.00        27\n","\n","\n","\n","    accuracy                           0.33     10000\n","\n","   macro avg       0.21      0.20      0.20     10000\n","\n","weighted avg       0.32      0.33      0.32     10000\n","\n","\n","\n","TF-IDF accuracy is: 0.3046\n","\n","              precision    recall  f1-score   support\n","\n","\n","\n","           0       0.40      0.38      0.39      2183\n","\n","           1       0.39      0.27      0.32      2093\n","\n","           2       0.34      0.30      0.32      1288\n","\n","           3       0.33      0.34      0.33      1314\n","\n","           4       0.45      0.45      0.45       941\n","\n","           5       0.13      0.21      0.16       516\n","\n","           6       0.12      0.19      0.14       421\n","\n","           7       0.16      0.22      0.18       435\n","\n","           8       0.23      0.36      0.28       332\n","\n","           9       0.02      0.02      0.02       203\n","\n","          10       0.04      0.04      0.04       204\n","\n","          11       0.03      0.02      0.03        43\n","\n","          12       0.00      0.00      0.00        27\n","\n","\n","\n","    accuracy                           0.30     10000\n","\n","   macro avg       0.20      0.21      0.20     10000\n","\n","weighted avg       0.33      0.30      0.31     10000\n","\n","\n"]}],"source":["#Fitting on extracted feature using bag of words.\n","#Note:Here since class imbalance exists so we set class weight to balanced,so that the algo can give more importance to minority class automatically.For more info check scikit-learn docs.\n","clf_bow_log=LogisticRegression(class_weight=\"balanced\",solver=\"sag\",multi_class=\"multinomial\") \n","clf_bow_log.fit(X_bow_train,Y_train)\n","y_pred_bow=clf_bow_log.predict(X_bow_test)\n","print(\"Bag of Words Accuracy:\",accuracy_score(Y_test,y_pred_bow))\n","print(classification_report(Y_test,y_pred_bow))\n","\n","#Fitting on TF-IDF extracted feature.\n","clf_tf_log=LogisticRegression(class_weight=\"balanced\")\n","clf_tf_log.fit(X_tf_train,Y_train)\n","y_pred_tf=clf_tf_log.predict(X_tf_test)\n","print(\"TF-IDF accuracy is:\",accuracy_score(Y_test,y_pred_tf))\n","print(classification_report(Y_test,y_pred_tf))"]},{"cell_type":"markdown","metadata":{},"source":["***Now Classifying using SVM with rbf kernel.***"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Bag of Words Accuracy: 0.3316\n","\n","              precision    recall  f1-score   support\n","\n","\n","\n","           0       0.38      0.45      0.41      2183\n","\n","           1       0.31      0.46      0.37      2093\n","\n","           2       0.32      0.34      0.33      1288\n","\n","           3       0.31      0.29      0.30      1314\n","\n","           4       0.46      0.37      0.41       941\n","\n","           5       0.19      0.14      0.16       516\n","\n","           6       0.14      0.09      0.11       421\n","\n","           7       0.23      0.07      0.10       435\n","\n","           8       0.39      0.19      0.26       332\n","\n","           9       0.05      0.01      0.02       203\n","\n","          10       0.03      0.00      0.01       204\n","\n","          11       0.00      0.00      0.00        43\n","\n","          12       0.00      0.00      0.00        27\n","\n","\n","\n","    accuracy                           0.33     10000\n","\n","   macro avg       0.22      0.19      0.19     10000\n","\n","weighted avg       0.32      0.33      0.32     10000\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["/home/saikat/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","\n","/home/saikat/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","\n","/home/saikat/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"name":"stdout","output_type":"stream","text":["TF-IDF accuracy is: 0.33\n","\n","              precision    recall  f1-score   support\n","\n","\n","\n","           0       0.30      0.66      0.41      2183\n","\n","           1       0.33      0.38      0.35      2093\n","\n","           2       0.38      0.26      0.31      1288\n","\n","           3       0.35      0.23      0.28      1314\n","\n","           4       0.51      0.34      0.41       941\n","\n","           5       0.27      0.08      0.13       516\n","\n","           6       0.10      0.02      0.03       421\n","\n","           7       0.23      0.02      0.04       435\n","\n","           8       0.45      0.14      0.21       332\n","\n","           9       0.11      0.02      0.03       203\n","\n","          10       0.00      0.00      0.00       204\n","\n","          11       0.00      0.00      0.00        43\n","\n","          12       0.00      0.00      0.00        27\n","\n","\n","\n","    accuracy                           0.33     10000\n","\n","   macro avg       0.23      0.17      0.17     10000\n","\n","weighted avg       0.32      0.33      0.30     10000\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["/home/saikat/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","\n","/home/saikat/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","\n","/home/saikat/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}],"source":["clf_bow_svm=SVC(class_weight=\"balanced\",random_state=42)\n","clf_bow_svm.fit(X_bow_train,Y_train)\n","y_pred_bow=clf_bow_svm.predict(X_bow_test)\n","print(\"Bag of Words Accuracy:\",accuracy_score(Y_test,y_pred_bow))\n","print(classification_report(Y_test,y_pred_bow))\n","\n","#Fitting on TF-IDF extracted feature.\n","clf_tf_svm=SVC(class_weight=\"balanced\")\n","clf_tf_svm.fit(X_tf_train,Y_train)\n","y_pred_tf=clf_tf_svm.predict(X_tf_test)\n","print(\"TF-IDF accuracy is:\",accuracy_score(Y_test,y_pred_tf))\n","print(classification_report(Y_test,y_pred_tf))"]},{"cell_type":"markdown","metadata":{},"source":["***As we have not addressed the data imbalancing problem so the result is inevitably very poor.In future will update the notebook and address this issue***"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T16:06:30.036946Z","iopub.status.busy":"2024-07-06T16:06:30.036556Z","iopub.status.idle":"2024-07-06T16:06:30.043044Z","shell.execute_reply":"2024-07-06T16:06:30.041774Z","shell.execute_reply.started":"2024-07-06T16:06:30.036916Z"},"trusted":true},"outputs":[],"source":["import gensim\n","from gensim.models import Word2Vec\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["***Tokenizing the processed tweets column.***"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T16:01:10.708977Z","iopub.status.busy":"2024-07-06T16:01:10.708529Z","iopub.status.idle":"2024-07-06T16:01:19.108192Z","shell.execute_reply":"2024-07-06T16:01:19.107130Z","shell.execute_reply.started":"2024-07-06T16:01:10.708939Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>sentiment</th>\n","      <th>author</th>\n","      <th>content</th>\n","      <th>processesed tweets</th>\n","      <th>processed tweets tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1956967341</td>\n","      <td>9</td>\n","      <td>xoshayzers</td>\n","      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n","      <td>tiffanylue i know i was listenin to bad habit ...</td>\n","      <td>[tiffanylue, i, know, i, was, listenin, to, ba...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1956967666</td>\n","      <td>3</td>\n","      <td>wannamama</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","      <td>layin n bed with a headache ughhhh waitin on y...</td>\n","      <td>[layin, n, bed, with, a, headache, ughhhh, wai...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1956967696</td>\n","      <td>3</td>\n","      <td>coolfunky</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","      <td>funeral ceremony gloomy friday</td>\n","      <td>[funeral, ceremony, gloomy, friday]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1956967789</td>\n","      <td>10</td>\n","      <td>czareaquino</td>\n","      <td>wants to hang out with friends SOON!</td>\n","      <td>wants to hang out with friends soon</td>\n","      <td>[wants, to, hang, out, with, friends, soon]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1956968416</td>\n","      <td>0</td>\n","      <td>xkilljoyx</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","      <td>dannycastillo we want to trade with someone wh...</td>\n","      <td>[dannycastillo, we, want, to, trade, with, som...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     tweet_id  sentiment       author  \\\n","0  1956967341          9   xoshayzers   \n","1  1956967666          3    wannamama   \n","2  1956967696          3    coolfunky   \n","3  1956967789         10  czareaquino   \n","4  1956968416          0    xkilljoyx   \n","\n","                                             content  \\\n","0  @tiffanylue i know  i was listenin to bad habi...   \n","1  Layin n bed with a headache  ughhhh...waitin o...   \n","2                Funeral ceremony...gloomy friday...   \n","3               wants to hang out with friends SOON!   \n","4  @dannycastillo We want to trade with someone w...   \n","\n","                                  processesed tweets  \\\n","0  tiffanylue i know i was listenin to bad habit ...   \n","1  layin n bed with a headache ughhhh waitin on y...   \n","2                     funeral ceremony gloomy friday   \n","3                wants to hang out with friends soon   \n","4  dannycastillo we want to trade with someone wh...   \n","\n","                             processed tweets tokens  \n","0  [tiffanylue, i, know, i, was, listenin, to, ba...  \n","1  [layin, n, bed, with, a, headache, ughhhh, wai...  \n","2                [funeral, ceremony, gloomy, friday]  \n","3        [wants, to, hang, out, with, friends, soon]  \n","4  [dannycastillo, we, want, to, trade, with, som...  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["data['processed tweets tokens']=data['processesed tweets'].apply(word_tokenize)\n","\n","data.head()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T16:14:38.927890Z","iopub.status.busy":"2024-07-06T16:14:38.927474Z","iopub.status.idle":"2024-07-06T16:14:44.757434Z","shell.execute_reply":"2024-07-06T16:14:44.756157Z","shell.execute_reply.started":"2024-07-06T16:14:38.927852Z"},"trusted":true},"outputs":[],"source":["#Training Word2vec model.\n","model=Word2Vec(sentences=data['processed tweets tokens'],workers=4,vector_size=1000)"]},{"cell_type":"markdown","metadata":{},"source":["***Creating a function use the word2vec model and for getting the vector features.***"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T16:14:49.039975Z","iopub.status.busy":"2024-07-06T16:14:49.039566Z","iopub.status.idle":"2024-07-06T16:14:49.046789Z","shell.execute_reply":"2024-07-06T16:14:49.045502Z","shell.execute_reply.started":"2024-07-06T16:14:49.039942Z"},"trusted":true},"outputs":[],"source":["def create_features(tweet:list):\n","    '''This function creates word to vector list and the takes mean of the features of words in a \n","    sentence.Because not all tweets will be of same length'''\n","    word_embeddings = [model.wv[word] for word in tweet if word in model.wv]\n","    if not word_embeddings:\n","        return np.zeros(model.vector_size)\n","\n","    return np.mean(word_embeddings, axis=0)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T16:14:52.845930Z","iopub.status.busy":"2024-07-06T16:14:52.845517Z","iopub.status.idle":"2024-07-06T16:14:56.814390Z","shell.execute_reply":"2024-07-06T16:14:56.813163Z","shell.execute_reply.started":"2024-07-06T16:14:52.845896Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(40000, 1000)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["tw_vector=[]\n","for tweet in data['processed tweets tokens'] :\n","    tw_vector.append(create_features(tweet))\n","\n","tw_vector=np.array(tw_vector)    \n","    \n","tw_vector.shape  "]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T16:15:00.137537Z","iopub.status.busy":"2024-07-06T16:15:00.137104Z","iopub.status.idle":"2024-07-06T16:15:00.179237Z","shell.execute_reply":"2024-07-06T16:15:00.177953Z","shell.execute_reply.started":"2024-07-06T16:15:00.137502Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>sentiment</th>\n","      <th>author</th>\n","      <th>content</th>\n","      <th>processesed tweets</th>\n","      <th>processed tweets tokens</th>\n","      <th>vectorized tweet feature</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1956967341</td>\n","      <td>9</td>\n","      <td>xoshayzers</td>\n","      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n","      <td>tiffanylue i know i was listenin to bad habit ...</td>\n","      <td>[tiffanylue, i, know, i, was, listenin, to, ba...</td>\n","      <td>[0.037860047072172165, 0.02605574205517769, 0....</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1956967666</td>\n","      <td>3</td>\n","      <td>wannamama</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","      <td>layin n bed with a headache ughhhh waitin on y...</td>\n","      <td>[layin, n, bed, with, a, headache, ughhhh, wai...</td>\n","      <td>[0.10870418697595596, 0.16994205117225647, 0.1...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1956967696</td>\n","      <td>3</td>\n","      <td>coolfunky</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","      <td>funeral ceremony gloomy friday</td>\n","      <td>[funeral, ceremony, gloomy, friday]</td>\n","      <td>[0.008656064048409462, 0.09180052578449249, 0....</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1956967789</td>\n","      <td>10</td>\n","      <td>czareaquino</td>\n","      <td>wants to hang out with friends SOON!</td>\n","      <td>wants to hang out with friends soon</td>\n","      <td>[wants, to, hang, out, with, friends, soon]</td>\n","      <td>[0.08795052021741867, 0.2231498807668686, 0.13...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1956968416</td>\n","      <td>0</td>\n","      <td>xkilljoyx</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","      <td>dannycastillo we want to trade with someone wh...</td>\n","      <td>[dannycastillo, we, want, to, trade, with, som...</td>\n","      <td>[0.12112851440906525, 0.11499065905809402, 0.1...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     tweet_id  sentiment       author  \\\n","0  1956967341          9   xoshayzers   \n","1  1956967666          3    wannamama   \n","2  1956967696          3    coolfunky   \n","3  1956967789         10  czareaquino   \n","4  1956968416          0    xkilljoyx   \n","\n","                                             content  \\\n","0  @tiffanylue i know  i was listenin to bad habi...   \n","1  Layin n bed with a headache  ughhhh...waitin o...   \n","2                Funeral ceremony...gloomy friday...   \n","3               wants to hang out with friends SOON!   \n","4  @dannycastillo We want to trade with someone w...   \n","\n","                                  processesed tweets  \\\n","0  tiffanylue i know i was listenin to bad habit ...   \n","1  layin n bed with a headache ughhhh waitin on y...   \n","2                     funeral ceremony gloomy friday   \n","3                wants to hang out with friends soon   \n","4  dannycastillo we want to trade with someone wh...   \n","\n","                             processed tweets tokens  \\\n","0  [tiffanylue, i, know, i, was, listenin, to, ba...   \n","1  [layin, n, bed, with, a, headache, ughhhh, wai...   \n","2                [funeral, ceremony, gloomy, friday]   \n","3        [wants, to, hang, out, with, friends, soon]   \n","4  [dannycastillo, we, want, to, trade, with, som...   \n","\n","                            vectorized tweet feature  \n","0  [0.037860047072172165, 0.02605574205517769, 0....  \n","1  [0.10870418697595596, 0.16994205117225647, 0.1...  \n","2  [0.008656064048409462, 0.09180052578449249, 0....  \n","3  [0.08795052021741867, 0.2231498807668686, 0.13...  \n","4  [0.12112851440906525, 0.11499065905809402, 0.1...  "]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["data['vectorized tweet feature']=list(tw_vector)\n","data.head()"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T16:15:04.005517Z","iopub.status.busy":"2024-07-06T16:15:04.005097Z","iopub.status.idle":"2024-07-06T16:15:04.135110Z","shell.execute_reply":"2024-07-06T16:15:04.133984Z","shell.execute_reply.started":"2024-07-06T16:15:04.005483Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set shape is:(30000, 1000) and test set is :(10000, 1000)\n"]}],"source":["Y=data['sentiment']\n","X_wv_train,X_wv_test,Y_train,Y_test=train_test_split(tw_vector,Y,test_size=0.25,random_state=42)\n","\n","print(f'Training set shape is:{X_wv_train.shape} and test set is :{X_wv_test.shape}')"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T16:15:09.074104Z","iopub.status.busy":"2024-07-06T16:15:09.073650Z","iopub.status.idle":"2024-07-06T16:17:49.418410Z","shell.execute_reply":"2024-07-06T16:17:49.413172Z","shell.execute_reply.started":"2024-07-06T16:15:09.074070Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Word2Vec Accuracy: 0.1264\n","              precision    recall  f1-score   support\n","\n","           0       0.34      0.06      0.10      2183\n","           1       0.32      0.06      0.11      2093\n","           2       0.31      0.14      0.20      1288\n","           3       0.24      0.09      0.13      1314\n","           4       0.32      0.40      0.35       941\n","           5       0.11      0.14      0.13       516\n","           6       0.07      0.09      0.08       421\n","           7       0.09      0.18      0.12       435\n","           8       0.07      0.20      0.11       332\n","           9       0.02      0.06      0.03       203\n","          10       0.03      0.19      0.06       204\n","          11       0.01      0.35      0.03        43\n","          12       0.00      0.15      0.01        27\n","\n","    accuracy                           0.13     10000\n","   macro avg       0.15      0.16      0.11     10000\n","weighted avg       0.26      0.13      0.14     10000\n","\n"]}],"source":["#Fitting Logistic on word2vec extracted feature.\n","clf_wv_log=LogisticRegression(class_weight=\"balanced\",solver=\"sag\",multi_class=\"multinomial\") \n","clf_wv_log.fit(X_wv_train,Y_train)\n","y_pred_wv=clf_wv_log.predict(X_wv_test)\n","print(\"Word2Vec Accuracy:\",accuracy_score(Y_test,y_pred_wv))\n","print(classification_report(Y_test,y_pred_wv))"]},{"cell_type":"markdown","metadata":{},"source":["***Here from the recall and f1 score the effect of data imbalance is clearly visible.***\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T16:24:11.357998Z","iopub.status.busy":"2024-07-06T16:24:11.356893Z","iopub.status.idle":"2024-07-06T16:57:02.357056Z","shell.execute_reply":"2024-07-06T16:57:02.355583Z","shell.execute_reply.started":"2024-07-06T16:24:11.357960Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Word2Vec Accuracy: 0.137\n","              precision    recall  f1-score   support\n","\n","           0       0.41      0.09      0.14      2183\n","           1       0.33      0.07      0.12      2093\n","           2       0.33      0.15      0.21      1288\n","           3       0.23      0.09      0.13      1314\n","           4       0.32      0.39      0.36       941\n","           5       0.11      0.15      0.12       516\n","           6       0.07      0.12      0.09       421\n","           7       0.08      0.15      0.10       435\n","           8       0.07      0.24      0.11       332\n","           9       0.03      0.07      0.04       203\n","          10       0.03      0.15      0.05       204\n","          11       0.01      0.33      0.02        43\n","          12       0.00      0.15      0.01        27\n","\n","    accuracy                           0.14     10000\n","   macro avg       0.16      0.17      0.12     10000\n","weighted avg       0.28      0.14      0.15     10000\n","\n"]}],"source":["#Fitting SVM on Word2Vec extracted feature.\n","clf_wv_svm=SVC(class_weight=\"balanced\",random_state=42)\n","clf_wv_svm.fit(X_wv_train,Y_train)\n","y_pred_wv=clf_wv_svm.predict(X_wv_test)\n","print(\"Word2Vec Accuracy:\",accuracy_score(Y_test,y_pred_wv))\n","print(classification_report(Y_test,y_pred_wv))"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T16:18:41.230227Z","iopub.status.busy":"2024-07-06T16:18:41.229239Z","iopub.status.idle":"2024-07-06T16:18:41.410617Z","shell.execute_reply":"2024-07-06T16:18:41.409445Z","shell.execute_reply.started":"2024-07-06T16:18:41.230188Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-07-06T16:18:43.073872Z","iopub.status.busy":"2024-07-06T16:18:43.073459Z","iopub.status.idle":"2024-07-06T16:22:04.046741Z","shell.execute_reply":"2024-07-06T16:22:04.045598Z","shell.execute_reply.started":"2024-07-06T16:18:43.073840Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Forest accuracy on Word2vec is: 0.277\n","              precision    recall  f1-score   support\n","\n","           0       0.30      0.48      0.37      2183\n","           1       0.26      0.53      0.35      2093\n","           2       0.28      0.20      0.23      1288\n","           3       0.19      0.07      0.10      1314\n","           4       0.39      0.25      0.30       941\n","           5       0.04      0.00      0.00       516\n","           6       0.05      0.00      0.00       421\n","           7       0.17      0.01      0.02       435\n","           8       0.38      0.02      0.03       332\n","           9       0.04      0.02      0.03       203\n","          10       0.03      0.00      0.01       204\n","          11       0.00      0.00      0.00        43\n","          12       0.00      0.00      0.00        27\n","\n","    accuracy                           0.28     10000\n","   macro avg       0.16      0.12      0.11     10000\n","weighted avg       0.24      0.28      0.23     10000\n","\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["#Fitting Random Forest on Word2Vec extracted feature.\n","clf_wv_rf=RandomForestClassifier(class_weight=\"balanced\",random_state=42)\n","clf_wv_rf.fit(X_wv_train,Y_train)\n","y_pred_wv=clf_wv_rf.predict(X_wv_test)\n","print(\"Random Forest accuracy on Word2vec is:\",accuracy_score(Y_test,y_pred_wv))\n","print(classification_report(Y_test,y_pred_wv))"]},{"cell_type":"markdown","metadata":{},"source":["***Here from the recall and f1 score the effect of data imbalance is clearly visible.***"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5345391,"sourceId":8882900,"sourceType":"datasetVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
